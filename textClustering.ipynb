{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 75: expected 17 fields, saw 18\\nSkipping line 106: expected 17 fields, saw 18\\nSkipping line 138: expected 17 fields, saw 18\\nSkipping line 165: expected 17 fields, saw 18\\nSkipping line 166: expected 17 fields, saw 19\\nSkipping line 187: expected 17 fields, saw 19\\nSkipping line 261: expected 17 fields, saw 18\\nSkipping line 269: expected 17 fields, saw 19\\nSkipping line 339: expected 17 fields, saw 19\\nSkipping line 702: expected 17 fields, saw 19\\nSkipping line 731: expected 17 fields, saw 18\\nSkipping line 773: expected 17 fields, saw 18\\nSkipping line 844: expected 17 fields, saw 19\\nSkipping line 1262: expected 17 fields, saw 18\\nSkipping line 1293: expected 17 fields, saw 18\\nSkipping line 1323: expected 17 fields, saw 20\\nSkipping line 1577: expected 17 fields, saw 18\\nSkipping line 1685: expected 17 fields, saw 19\\nSkipping line 1967: expected 17 fields, saw 18\\nSkipping line 1973: expected 17 fields, saw 18\\nSkipping line 2102: expected 17 fields, saw 18\\nSkipping line 2184: expected 17 fields, saw 19\\nSkipping line 2461: expected 17 fields, saw 19\\nSkipping line 2488: expected 17 fields, saw 18\\nSkipping line 2703: expected 17 fields, saw 18\\nSkipping line 2724: expected 17 fields, saw 18\\nSkipping line 2793: expected 17 fields, saw 18\\nSkipping line 3061: expected 17 fields, saw 18\\nSkipping line 3156: expected 17 fields, saw 18\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_dac.csv', sep = ',', header = 0, error_bad_lines=False, date_parser=['posted_at'])\n",
    "df = df[df.named_id.notnull() == True ]\n",
    "df.nb_follower = df.nb_follower.astype('int32')\n",
    "df.nb_following = df.nb_following.astype('int32')\n",
    "df.length = df.length.astype('int32')\n",
    "df.orthographe = df.orthographe.astype('float64')\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à l'analyse des features que nous avons effectué dans 'pretraitement' nou allons maintenant procéder à des analyses sur les textes des tweets pour voir si des tendances se dégagent et ce que l'on peut en tirer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par définir une fonction permettant de tokenizer les tweets que nous souhaitons analyser. Nous allons d'abord supprimer tout les hyperliens et mention d'utilisateur qui risque de perturber notre analyse. Nous allons aussi supprimer les stopwords et découper les hastags en fonction des majuscules (#GiletJaune devient 'Gilet Jaune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fr_core_news_md\n",
    "nlp = fr_core_news_md.load()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from Emojilist import emojilist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stopwords_fr = stopwords.words('french')\n",
    "\n",
    "\n",
    "def tokenize(message):\n",
    "        message = re.sub(r'(?:\\@|https?\\://)\\S+', '', message)  # remove links and @username\n",
    "        message = re.sub(r'[^\\w\\s]', ' ', message)  # remove non-alphanumeric characters\n",
    "        doc = nlp (message)\n",
    "        stopWords = new_stopwords\n",
    "        hashtag = False\n",
    "        list = [str(token) for token in doc]\n",
    "        tokens = []\n",
    "        for elt in list :\n",
    "            #remove stopwords\n",
    "            if elt not in stopWords :\n",
    "                #couper les lettres qui se repètent plus de 2fois de suite\n",
    "                for i in range(len(elt)-1,1, -1):\n",
    "                    if elt[i] == elt [i-1] and elt[i] == elt [i-2] :\n",
    "                        elt = elt[:i]+elt[i+1:]\n",
    "                #On coupe le mot si il s'agit du terme après un hastag\n",
    "                if hashtag == True :\n",
    "                    decoup = re.findall('[A-Z][^A-Z]*',elt)\n",
    "                    for word in decoup :\n",
    "                        tokens.append(word)\n",
    "                    hashtag = False\n",
    "                elif elt == '#' :\n",
    "                    hashtag = True\n",
    "                elif elt in emojilist :\n",
    "                    pass\n",
    "                else :\n",
    "                    tokens.append(elt)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "def cluster_texts_Kmeans( texts, k=5):\n",
    "        \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize,\n",
    "            max_features=1500,\n",
    "            max_df=0.7,\n",
    "            min_df=5,\n",
    "            lowercase=True)\n",
    "\n",
    "        tfidf_model = vectorizer.fit_transform(texts)\n",
    "        km_model = KMeans(n_clusters=k, init='k-means++', n_init=10)\n",
    "        km_model.fit_predict(tfidf_model)\n",
    "        \n",
    "        clustering = collections.defaultdict(list)\n",
    "\n",
    "        print(\"Top terms per cluster:\")\n",
    "        order_centroids = km_model.cluster_centers_.argsort()[:, ::-1]\n",
    "        #print(order_centroids)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        for i in range(k):\n",
    "            print(\"\\n Cluster %d:\" % i)\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                print(' %s' % terms[ind])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      " Cluster 0:\n",
      " a\n",
      "   \n",
      " les\n",
      " fait\n",
      " tout\n",
      " plus\n",
      " faire\n",
      " si\n",
      " bien\n",
      " rien\n",
      "\n",
      " Cluster 1:\n",
      "   \n",
      " plus\n",
      " bien\n",
      " merci\n",
      " fait\n",
      " \n",
      "\n",
      " si\n",
      " quand\n",
      " trop\n",
      " non\n",
      "\n",
      " Cluster 2:\n",
      "   \n",
      " les\n",
      " trop\n",
      " vrai\n",
      " via\n",
      " ah\n",
      " dit\n",
      " merci\n",
      " live\n",
      " père\n",
      "\n",
      " Cluster 3:\n",
      " ça\n",
      "   \n",
      " si\n",
      " fait\n",
      " a\n",
      " les\n",
      " comme\n",
      " bien\n",
      " plus\n",
      " va\n",
      "\n",
      " Cluster 4:\n",
      " les\n",
      "   \n",
      " ils\n",
      " tous\n",
      " gens\n",
      " plus\n",
      " \n",
      "\n",
      " là\n",
      " comme\n",
      " quand\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster_texts_Kmeans(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons qu'il reste beaucoup de stopwords dans les mots les plus fréquents. Nous allons donc compléter la liste de stopwords pour qu'elle soit plus complète que la liste de base présente dans ntlk et modifier la fonction tokenize pour qu'elle prenne cette nouvelle liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = stopwords.words('french') + ['oui','non','merci','a','bah','toujours','va','dit','alors','juste','sujet','au','la','sur','aucuns','le','ta','aussi','les','tandis',\n",
    "'autre','leur','tellement','avant','là','tels','avec','ma','tes','avoir','maintenant','ton','bon','mais','tous',\n",
    "'car','mes','tout','ce','mine','trop','cela','moins','très','ces','mon','tu','ceux','mot','voient','chaque','même','vont',\n",
    "'ci','ni','votre','comme','nommés','vous','comment','notre','vu','dans','nous','ça','des','ou','étaient','du','où','état',\n",
    "'dedans','par','étions','dehors','parce','été','depuis','pas','être','devrait','peut','doit','peu','donc','plupart',\n",
    "'dos','pour','début','pourquoi','elle','quand','elles','que','en','quel','encore','quelle','essai','quelles','est','quels',\n",
    "'et','qui','eu','sa','fait','sans','faites','ses','fois','seulement','font','si','hors','sien','ici','son','il','sont',\n",
    "'ils','sous','je','soyez','plus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      " Cluster 0:\n",
      "   \n",
      " 3\n",
      " via\n",
      " ah\n",
      " jamais\n",
      " no\n",
      " malaise\n",
      " live\n",
      " père\n",
      " pareil\n",
      "\n",
      " Cluster 1:\n",
      "   \n",
      " \n",
      "\n",
      " quoi\n",
      " cette\n",
      " mdrr\n",
      " faut\n",
      " 2\n",
      " mdr\n",
      " jamais\n",
      " ah\n",
      "\n",
      " Cluster 2:\n",
      " bien\n",
      "   \n",
      " sûr\n",
      " sais\n",
      " toutes\n",
      " deux\n",
      "  \n",
      "\n",
      " ah\n",
      " connu\n",
      " aime\n",
      "\n",
      " Cluster 3:\n",
      " france\n",
      "   \n",
      " honte\n",
      " entre\n",
      " années\n",
      " temps\n",
      " 5\n",
      " \n",
      "  \n",
      " 3\n",
      " bientôt\n",
      "\n",
      " Cluster 4:\n",
      " faire\n",
      "   \n",
      " vais\n",
      " faut\n",
      " plaisir\n",
      " rien\n",
      " envie\n",
      " partie\n",
      " 2019\n",
      " aller\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster_texts_Kmeans(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on obtient des termes plus pertinents pour qualifier les clusters. Néanmoins il nous reste à définir le nombre de cluster qu'il serait pertinent de faire car pour l'instant il est fixé par nous avec la méthode des K_means.\n",
    "\n",
    "Pour cela nous avons regardé les différents algorithme qui permettait de définir automatiquement le nombre de cluster basé sur la densité et nous avons choisi de nous interesser à la fonction DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def cluster_texts_DBSCAN( texts):\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize,\n",
    "            max_features=1500,\n",
    "            max_df=0.7,\n",
    "            min_df=5,\n",
    "            lowercase=True)\n",
    "\n",
    "        tfidf_model = vectorizer.fit_transform(texts)\n",
    "        db = DBSCAN(eps=0.5, min_samples=5)\n",
    "        db.fit_predict(tfidf_model)\n",
    "        labels = db.labels_\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "        print('Estimated number of clusters: %d' % n_clusters_)\n",
    "        print('Estimated number of noise points: %d' % n_noise_)\n",
    "        for j in range (n_clusters_) :\n",
    "            print(\"\\n Cluster %d:\" % j)\n",
    "            for i in range(len(labels)) :\n",
    "                if labels[i] == j :\n",
    "                    print('tweet : ' + str(df.text[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_texts_DBSCAN(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse des clusters réalisé avec DBSCAN nous montre que beaucoup de point (75%) sont classés comme bruit et seul 25% sont clusterisés avec les paramètres actuels. \n",
    "Si nous voulons que plus de points soient clusterisés il faudrait que nous diminuions le paramètres eps pour DBSCAN qui catégorise a partir de quelle distance on considère 2 points comme proches. Néanmoins si l'on fait ça alors la pertinence de nos clusters risque de diminuer alors qu'elle est déjà faible pour le cluster 0 (qui n'est pourtant pas le cluster poubelle).\n",
    "\n",
    "C'est pourquoi il faudrait mener des tests avec plusieurs valeurs de 'eps' et 'min_samples' pour voir quelle combinaison nous donne des clusters pertinents pour l'analyse du dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
