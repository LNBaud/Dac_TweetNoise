{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 75: expected 17 fields, saw 18\\nSkipping line 106: expected 17 fields, saw 18\\nSkipping line 138: expected 17 fields, saw 18\\nSkipping line 165: expected 17 fields, saw 18\\nSkipping line 166: expected 17 fields, saw 19\\nSkipping line 187: expected 17 fields, saw 19\\nSkipping line 261: expected 17 fields, saw 18\\nSkipping line 269: expected 17 fields, saw 19\\nSkipping line 339: expected 17 fields, saw 19\\nSkipping line 702: expected 17 fields, saw 19\\nSkipping line 731: expected 17 fields, saw 18\\nSkipping line 773: expected 17 fields, saw 18\\nSkipping line 844: expected 17 fields, saw 19\\nSkipping line 1262: expected 17 fields, saw 18\\nSkipping line 1293: expected 17 fields, saw 18\\nSkipping line 1323: expected 17 fields, saw 20\\nSkipping line 1577: expected 17 fields, saw 18\\nSkipping line 1685: expected 17 fields, saw 19\\nSkipping line 1967: expected 17 fields, saw 18\\nSkipping line 1973: expected 17 fields, saw 18\\nSkipping line 2102: expected 17 fields, saw 18\\nSkipping line 2184: expected 17 fields, saw 19\\nSkipping line 2461: expected 17 fields, saw 19\\nSkipping line 2488: expected 17 fields, saw 18\\nSkipping line 2703: expected 17 fields, saw 18\\nSkipping line 2724: expected 17 fields, saw 18\\nSkipping line 2793: expected 17 fields, saw 18\\nSkipping line 3061: expected 17 fields, saw 18\\nSkipping line 3156: expected 17 fields, saw 18\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_dac.csv', sep = ',', header = 0, error_bad_lines=False, date_parser=['posted_at'])\n",
    "df = df[df.named_id.notnull() == True ]\n",
    "df.nb_follower = df.nb_follower.astype('int32')\n",
    "df.nb_following = df.nb_following.astype('int32')\n",
    "df.length = df.length.astype('int32')\n",
    "df.orthographe = df.orthographe.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un second temps nous allons procéder à des analyses sur les textes des tweets pour voir si des tendances se dégagent et ce que l'on peut en tirer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par définir une fonction permettant de tokenizer les tweets que nous souhaitons analyser. Nous allons d'abord supprimer tout les hyperliens et mention d'utilisateur qui risque de perturber notre analyse. Nous allons aussi supprimer les stopwords et découper les hastags en fonction des majuscules (#GiletJaune devient 'Gilet Jaune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fr_core_news_md\n",
    "nlp = fr_core_news_md.load()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from Emojilist import emojilist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stopwords_fr = stopwords.words('french')\n",
    "\n",
    "\n",
    "def tokenize(message):\n",
    "        message = re.sub(r'(?:\\@|https?\\://)\\S+', '', message)  # remove links and @username\n",
    "        message = re.sub(r'[^\\w\\s]', ' ', message)  # remove non-alphanumeric characters\n",
    "        doc = nlp (message)\n",
    "        stopWords = new_stopwords\n",
    "        hashtag = False\n",
    "        list = [str(token) for token in doc]\n",
    "        tokens = []\n",
    "        for elt in list :\n",
    "            #remove stopwords\n",
    "            if elt not in stopWords :\n",
    "                #couper les lettres qui se repètent plus de 2fois de suite\n",
    "                for i in range(len(elt)-1,1, -1):\n",
    "                    if elt[i] == elt [i-1] and elt[i] == elt [i-2] :\n",
    "                        elt = elt[:i]+elt[i+1:]\n",
    "                #On coupe le mot si il s'agit du terme après un hastag\n",
    "                if hashtag == True :\n",
    "                    decoup = re.findall('[A-Z][^A-Z]*',elt)\n",
    "                    for word in decoup :\n",
    "                        tokens.append(word)\n",
    "                    hashtag = False\n",
    "                elif elt == '#' :\n",
    "                    hashtag = True\n",
    "                elif elt in emojilist :\n",
    "                    pass\n",
    "                else :\n",
    "                    tokens.append(elt)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "def cluster_texts_Kmeans( texts, k=5):\n",
    "        \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize,\n",
    "            max_features=1500,\n",
    "            max_df=0.7,\n",
    "            min_df=5,\n",
    "            lowercase=True)\n",
    "\n",
    "        tfidf_model = vectorizer.fit_transform(texts)\n",
    "        km_model = KMeans(n_clusters=k, init='k-means++', n_init=10)\n",
    "        km_model.fit_predict(tfidf_model)\n",
    "        \n",
    "        clustering = collections.defaultdict(list)\n",
    "\n",
    "        print(\"Top terms per cluster:\")\n",
    "        order_centroids = km_model.cluster_centers_.argsort()[:, ::-1]\n",
    "        #print(order_centroids)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        for i in range(k):\n",
    "            print(\"\\n Cluster %d:\" % i)\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                print(' %s' % terms[ind])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      " Cluster 0:\n",
      "   \n",
      " bien\n",
      " \n",
      "\n",
      " quoi\n",
      " via\n",
      " ah\n",
      " veux\n",
      " aime\n",
      " rien\n",
      " faut\n",
      "\n",
      " Cluster 1:\n",
      "   \n",
      " autres\n",
      " dis\n",
      " pseudo\n",
      " nouveau\n",
      " 3\n",
      " oeil\n",
      " parfait\n",
      " pouvez\n",
      " changer\n",
      "\n",
      " Cluster 2:\n",
      " faire\n",
      "   \n",
      " vais\n",
      " faut\n",
      " plaisir\n",
      " rien\n",
      " partie\n",
      " aller\n",
      " genre\n",
      " envie\n",
      "\n",
      " Cluster 3:\n",
      " sais\n",
      "   \n",
      " bien\n",
      " demande\n",
      " imagine\n",
      " fou\n",
      " cours\n",
      " xd\n",
      " mdr\n",
      " gens\n",
      "\n",
      " Cluster 4:\n",
      " bien\n",
      " \n",
      "\n",
      " mdrr\n",
      " cette\n",
      " gens\n",
      " faut\n",
      " mdr\n",
      " jamais\n",
      " quoi\n",
      " 2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster_texts_Kmeans(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons qu'il reste beaucoup de stopwords dans les mots les plus fréquents. Nous allons donc compléter la liste de stopwords pour qu'elle soit plus complète que la liste de base présente dans ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = stopwords.words('french') + ['oui','non','merci','a','bah','toujours','va','dit','alors','juste','sujet','au','la','sur','aucuns','le','ta','aussi','les','tandis',\n",
    "'autre','leur','tellement','avant','là','tels','avec','ma','tes','avoir','maintenant','ton','bon','mais','tous',\n",
    "'car','mes','tout','ce','mine','trop','cela','moins','très','ces','mon','tu','ceux','mot','voient','chaque','même','vont',\n",
    "'ci','ni','votre','comme','nommés','vous','comment','notre','vu','dans','nous','ça','des','ou','étaient','du','où','état',\n",
    "'dedans','par','étions','dehors','parce','été','depuis','pas','être','devrait','peut','doit','peu','donc','plupart',\n",
    "'dos','pour','début','pourquoi','elle','quand','elles','que','en','quel','encore','quelle','essai','quelles','est','quels',\n",
    "'et','qui','eu','sa','fait','sans','faites','ses','fois','seulement','font','si','hors','sien','ici','son','il','sont',\n",
    "'ils','sous','je','soyez','plus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " plus\n",
      "   \n",
      " non\n",
      " vraiment\n",
      "  \n",
      "\n",
      " sait\n",
      " france\n",
      " 5\n",
      " jamais\n",
      " actuellement\n",
      "Cluster 1:\n",
      " bien\n",
      "   \n",
      " sûr\n",
      " sais\n",
      " toutes\n",
      " deux\n",
      "  \n",
      "\n",
      " ah\n",
      " connu\n",
      " aime\n",
      "Cluster 2:\n",
      "   \n",
      " 3\n",
      " via\n",
      " ah\n",
      " jamais\n",
      " no\n",
      " malaise\n",
      " live\n",
      " père\n",
      " pareil\n",
      "Cluster 3:\n",
      " \n",
      "\n",
      " non\n",
      "   \n",
      "  \n",
      "\n",
      " 00\n",
      " \n",
      " \n",
      " mdr\n",
      " bonjour\n",
      " dis\n",
      " peux\n",
      "Cluster 4:\n",
      "   \n",
      " faire\n",
      " quoi\n",
      " faut\n",
      " cette\n",
      " mdrr\n",
      " mdr\n",
      " 2\n",
      " sais\n",
      " rien\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster_texts_Kmeans(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on obtient des termes plus pertinents pour qualifier les clusters. Néanmoins il nous reste donc a définir le nombre de cluster qu'il serait pertinent de faire car pour l'instant il est fixé par nous.\n",
    "\n",
    "Pour cela nous allons utiliser la méthode DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def cluster_texts_DBSCAN( texts):\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize,\n",
    "            max_features=1500,\n",
    "            max_df=0.7,\n",
    "            min_df=5,\n",
    "            lowercase=True)\n",
    "\n",
    "        tfidf_model = vectorizer.fit_transform(texts)\n",
    "        km_model = DBSCAN(eps=0.5, min_samples=5)\n",
    "        print(tokenize(texts[0]))\n",
    "        print(tfidf_model)\n",
    "        km_model.fit_predict(tfidf_model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', ' ', 'frenchweb', '  ', 'INSIDERS', ' ', 'Robots', 'livreurs', '  ', 'SoftBank', 'mise', 'près', 'milliard', 'dollars', 'Nuro', ' ']\n",
      "  (0, 554)\t0.6058126965772836\n",
      "  (0, 9)\t0.35353530607209843\n",
      "  (0, 510)\t0.7127438277714285\n",
      "  (1, 245)\t0.44840990704972183\n",
      "  (1, 660)\t0.6555957974788901\n",
      "  (1, 498)\t0.6075546934948969\n",
      "  (2, 9)\t0.5815238990989503\n",
      "  (2, 365)\t0.8135293201703022\n",
      "  (3, 381)\t0.7259073321058642\n",
      "  (3, 137)\t0.6877925160940229\n",
      "  (4, 451)\t0.548550952226257\n",
      "  (4, 657)\t0.4609726875835015\n",
      "  (4, 475)\t0.4543181679508845\n",
      "  (4, 216)\t0.5293307438487418\n",
      "  (5, 634)\t0.5385900560495686\n",
      "  (5, 86)\t0.5957855115410758\n",
      "  (5, 306)\t0.5957855115410758\n",
      "  (6, 247)\t0.4750147246387601\n",
      "  (6, 606)\t0.6995081379342027\n",
      "  (6, 164)\t0.5339001557783881\n",
      "  (7, 9)\t0.24812021320330557\n",
      "  (7, 130)\t0.9687292499971012\n",
      "  (8, 9)\t0.37405240896457803\n",
      "  (8, 6)\t0.44890291513159025\n",
      "  (8, 350)\t0.5122751511966681\n",
      "  :\t:\n",
      "  (3346, 327)\t0.32197869053399153\n",
      "  (3346, 303)\t0.3933505360740032\n",
      "  (3346, 1)\t0.34497017433208493\n",
      "  (3347, 222)\t1.0\n",
      "  (3348, 9)\t0.19581138436498066\n",
      "  (3348, 164)\t0.6272376381606084\n",
      "  (3348, 294)\t0.7538108828000372\n",
      "  (3349, 229)\t0.6875540473950164\n",
      "  (3349, 85)\t0.7261332053492195\n",
      "  (3350, 9)\t0.19901813573275828\n",
      "  (3350, 644)\t0.6342051369367805\n",
      "  (3350, 297)\t0.7471115217505729\n",
      "  (3352, 9)\t0.131381585851679\n",
      "  (3352, 512)\t0.5513931880689178\n",
      "  (3352, 14)\t0.4328221246664085\n",
      "  (3352, 674)\t0.4328221246664085\n",
      "  (3352, 406)\t0.5513931880689178\n",
      "  (3353, 86)\t0.3006067490095716\n",
      "  (3353, 306)\t0.3006067490095716\n",
      "  (3353, 645)\t0.267818660452911\n",
      "  (3353, 421)\t0.691186844188291\n",
      "  (3353, 18)\t0.5194276431169528\n",
      "  (3354, 247)\t0.4397923776985849\n",
      "  (3354, 544)\t0.6476396019840143\n",
      "  (3354, 187)\t0.6222102622588386\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster_texts_DBSCAN(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
